[{"title":"hello, eyeryone！","url":"/2025/05/24/hello-world/","content":"你好，欢迎来到我的博客！\n我是一名来自中国的人工智能工程师，目前就职于一家初创公司，专注于人工智能生成内容（AIGC）和大模型相关的技术研究与应用。\n我本科毕业于中国矿业大学（北京），在校期间便对计算机视觉与自然语言处理产生了浓厚兴趣。如今，我主要从事大规模预训练模型、文本生成、图像生成等方向的研发工作。\n在这个博客中，我将记录一些工作中的技术总结、工程实践、学习心得，以及对 AI 行业发展的思考。如果你也对 AIGC、大模型或 AI 工程感兴趣，欢迎一起交流探讨！\n\n\nStay hungry, stay curious.\n\n","categories":["博客"],"tags":["自我介绍","AIGC","大模型"]},{"title":"VAE学习笔记1","url":"/2025/06/22/vae(1)/","content":"什么是变分自编码器 （VAE） ？VAE是一种生成模型，包含两个子模型：编码器（encoder）和解码器（decoder）。\n编码器学习图像数据，将图像数据转换成潜在空间的概率分布（通常为高斯分布），再从这个分布中采样一个潜在向量，作为解码器的输入。解码器从潜在向量上采样重建图像。\n一般的vae训练采用端到端的方式训练，输入图像，输出重建过后的图像。\n模型主体类似于U-Net结构，使用卷积层和残差神经网络等模块构建对称的解码器和编码器结构。\n训练完毕后的解码器可用于从正态分布采样后（解码器经过训练后，输出的潜在向量接近于正态分布，并且解码器是通过重采样学习，具有良好的噪声鲁棒性，因此可以直接使用正态分布采样），经过一系列变换，输出无条件约束的图像。\nvae的损失函数由两部分组成：\n1、重建损失\n衡量解码器输出与原始输入的相似度，一般使用MSE。可选用其他：L1, BCE等\n聚合方法：像素平均、求和\nrecon_loss=F.mse_loss(recon_x, x, reduction=&#x27;mean&#x27;) # 重建损失\n\n2、L1约束：KL散度\n聚合方法：像素平均、求和\nkl_loss=0.5*torch.mean(1+logvar-mu.pow(2)-logvar.exp()) # KL散度\n\n3、总损失\n重建损失和KL约束损失之间是互相对抗的学习，重建损失下降的时候，KL损失会上升，KL损失下降的时候，重建损失会上升。要让模型尽可能达到这两个空间中的平衡最优点，需要为模型建立起有效学习路径，使其充分学习重建能力的同时，让生成的潜在向量尽可能接近正态分布。通过引入一个参数γ，控制KL损失对重建损失的影响。\nloss=recon_loss+gamma*kl_loss\n\n退火多项实验证明，gamma为1，vae几乎无法建立起重建任务的学习路径，反而一直学习KL损失的下降，导致模型无法重建图像。对于这一个现象的一个解释是潜在变量过于快的变为正态分布，同时编码器学习不充分，重建图像的误差很大，导致解码器很难学习到从单一分布到正确图像的映射路径。\n因此，适当地对gamma进行退火，使模型在训练初期快速学习重建能力，减少模型训练失败的风险。\n一般的退火方法有：\n1、线性退火：γ&#x3D;γ+n∗α, n为迭代次数，α为退火速率,γ初始值在0-0.01之间\n2、循环退火。以n步骤为一个退火周期，在周期内将训练步数分为两个阶段，第一个阶段线性退火使得gamma增加到1，然后第二个阶段保持gamma为1，进入下一个周期时gamma重新设为0。\n相关的优化方法1、由于潜在变量最终的形态是正态分布，vae编码器输出和解码器输入的分布类型没有变化，模型的搜索空间在训练后期就只有一种。为使得达到最佳性能，可以为编码器和解码器分别加一个线性层，编码器通过一层线性层输出正态分布的参数，解码器通过一层线性层获得重采样映射后的输入，使得模型可以通过线性层线性变换成自己合适处理的分布。这个方法见于sd1.5的vae部分设计。\n2、如果输入图像已经处理成了(-1,1)区间，那么可以为解码器的最终输出添加tanh激活函数，使分布保持一致，加快收敛。\n训练时注意事项1、vae的两个损失函数是对抗关系，因此需要确定合理的退火策略，保证两者训练过程中量级相近。2、数据样本过少时，采用降低vae的潜在向量维度来减小模型可训练权重。3、图像的处理分辨率较大时，潜在向量的维度也要相应增大。\n","categories":["学习笔记"],"tags":["AIGC","生图","vae"]}]